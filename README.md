 # LoRA-MLflow Workflow (Text dataset)

For the text dataset, I followed the general workflow from:  
- [Kaggle notebook: NLP Fine-Tuning Sentiment Analysis](https://www.kaggle.com/code/yannicksteph/nlp-fine-tuning-sentiment-analysis)  
- [Hugging Face Tokenizer documentation](https://huggingface.co/docs/transformers/en/main_classes/tokenizer)  

### üîß Key Differences in This Project
- Used the **NSMC Korean dataset** instead of English datasets.  
- Fine-tuned the **`klue/roberta-base`** model.  
- Added **LoRA (Parameter-Efficient Fine-Tuning)** for efficient training.  
- Integrated **MLflow** to save models and track experiments directly in Google Colab.  

### üìä Workflow
1. Load dataset (`datasets` library).  
2. Tokenize with Hugging Face `AutoTokenizer`.  
3. Apply LoRA with `peft`.  
4. Train and evaluate with `Trainer`.  
5. Log metrics and models with **MLflow**.


#  Fine-Tuning ViT with LoRA on Tiny-ImageNet

This example follows the Hugging Face PEFT guide; I used the same code but with a different dataset (Tiny-ImageNet):  
üëâ [LoRA-based Methods (Hugging Face Docs)](https://huggingface.co/docs/peft/en/task_guides/lora_based_methods)



## ‚ö†Ô∏è Notes

- The hyperparameters in this repo are **not optimized for best performance**.  
